{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting on the Abalone dataset with Amazon SageMaker XGBoost Algorithm\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/SofianHamiti/amazon-sagemaker-abalone-experiments/main/media/manual.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you **organize, track, compare, and evaluate your machine learning experiments**.\n",
    "\n",
    "Machine learning is an iterative process. You need to experiment with multiple combinations of data, algorithm and parameters, all the while observing the impact of incremental changes on model accuracy. Over time this iterative experimentation can result in thousands of model training runs and model versions. This makes it hard to track the best performing models and their input configurations. It’s also difficult to compare active experiments with past experiments to identify opportunities for further incremental improvements.\n",
    "\n",
    "SageMaker Experiments **automatically tracks the inputs, parameters, configurations, and results of your iterations as trials. You can assign, group, and organize these trials into experiments**. SageMaker Experiments is integrated with Amazon SageMaker Studio providing a visual interface to browse your active and past experiments, compare trials on key performance metrics, and identify the best performing models.\n",
    "\n",
    "Because SageMaker Experiments enables tracking of all the steps and artifacts that went into creating a model, you can quickly revisit the origins of a model when you are troubleshooting issues in production, or auditing your models for compliance verifications.\n",
    "\n",
    "SageMaker Experiments comes with its own Experiments Python SDK which makes the analytics capabilities easily accessible in Amazon SageMaker Notebooks. You can install this by running `pip install sagemaker-experiments`. \n",
    "\n",
    "In this notebook, we will run a few training jobs with the XGBoost Estimator built-in into Amazon SageMaker and track some of its metrics via SageMaker Experiments. Note that in this notebook we only focus on Training and Tuning jobs, however SM Experiments is also compatible with pre-processing jobs via SageMaker Processing, batch inferences with SageMaker Batch Transform, and pipelines built with SageMaker Pipelines.\n",
    "\n",
    "Let's set-up the dependendencies that we will need throughout the notebook and some useful SageMaker helper function and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU sagemaker>=2.37.0\n",
    "!pip install -qU sagemaker-experiments>=0.1.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've run the above cell in a SageMaker Notebook, please reload the kernel, then advance to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import time\n",
    "import zipfile\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-inference-script-mode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading the [direct marketing dataset](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip) from the sample data s3 bucket and preprocess it. If you want to know more about the preprocessing, check the [xgboost direct marketing notebook](./xgboost_direct_marketing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip -O /tmp/bank-additional.zip\n",
    "\n",
    "with zipfile.ZipFile('/tmp/bank-additional.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/tmp/')\n",
    "    \n",
    "# Load the data into a DataFrame\n",
    "data = pd.read_csv('/tmp/bank-additional/bank-additional-full.csv')\n",
    "# Run some preprocessing\n",
    "data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)                                 # Indicator variable to capture when pdays takes a value of 999\n",
    "data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)   # Indicator for individuals not actively employed\n",
    "model_data = pd.get_dummies(data)                                                                  # Convert categorical variables to sets of indicators\n",
    "model_data = model_data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)\n",
    "# Train / test/ validation split\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   # Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "# Locally save the data\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('/tmp/train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('/tmp/validation.csv', index=False, header=False)\n",
    "\n",
    "print('Train and validation files written locally.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "train_path = sagemaker_session.upload_data('/tmp/train.csv', bucket=bucket, key_prefix=os.path.join(prefix, 'train/train.csv'))\n",
    "validation_path = sagemaker_session.upload_data('/tmp/validation.csv', bucket=bucket, key_prefix=os.path.join(prefix, 'validation/train.csv'))\n",
    "\n",
    "train_input = TrainingInput(train_path, content_type='text/csv')\n",
    "validation_input = TrainingInput(validation_path, content_type='text/csv')\n",
    "\n",
    "data_inputs = {\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Experiments: features\n",
    "\n",
    "Amazon SageMaker Experiments offers a structured organization scheme to help users group and organize their machine learning iterations. The top level entity, an **experiment**, is a collection of **trials** that are observed, compared, and evaluated as a group. A trial is a set of steps called **trial components**. Each trial component can include a combination of inputs such as datasets, algorithms, and parameters, and produce specific outputs such as models, metrics, datasets, and checkpoints. Examples of trial components are data pre-processing jobs, training jobs, and batch transform jobs.\n",
    "\n",
    "The goal of an experiment is to determine the trial that produces the best model. Multiple trials are performed, each one isolating and measuring the impact of a change to one or more inputs, while keeping the remaining inputs constant. By analyzing the trials, you can determine which features have the most effect on the model.\n",
    "\n",
    "Let's start by creating an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_experiment = Experiment.create(\n",
    "    experiment_name=f\"direct-marketing-{int(time.time())}\", \n",
    "    description=\"Using SM Experiments with the Direct Marketing dataset.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment that we've just created can also be visualized in the SageMaker Resources tab of SageMaker Studio. Just click on the SageMaker Resources button on the left, look for \"SageMaker Experiments and trials\" in the drop-down menu, and double-click on the `direct-marketing-XXXXXXXXXX` experiment. Since we have not created a trial yet, nothing will populate this menu for now. However, you can also check the *`Unassigned trial components`* which should be populated by the previous processing or training jobs you've run in the current account.\n",
    "\n",
    "### SageMaker Experiments: Trials and Training job\n",
    "\n",
    "Let's start simple and launch a training job for our XGBoost model. SageMaker will automatically create a **`Trial`** for us, or we can choose to create one ourselves to give it a name of our choice. To create a new Trial, its name must be unique to the current account. A Trial is also bound to a specific Experiment, which we've already created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = f\"direct-marketing-xgboost-{int(time.time())}\"\n",
    "trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=example_experiment.experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now refresh on the left the Experiments and trial SageMaker Resource tab, to see that a new Trial has been created. We can now directly train our XGBoost model and track it with SM Experiments and the Trial we have just created. To learn more about training, check out the [xgboost direct marketing notebook](./xgboost-direct-marketing.ipynb). What's different from the basic SageMaker training is that in the `fit()` call we actually provide an `experiment_config` value, which bounds the training job to the the Trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework='xgboost', \n",
    "    version='latest'\n",
    ")\n",
    "# Set-up the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "# Set the hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n",
    "    subsample=0.8, silent=0, objective='binary:logistic', num_round=500\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    inputs = data_inputs,\n",
    "    experiment_config={\n",
    "        \"TrialName\": trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"DirectMarketingTrainingXGBoost\", # If not provided, SageMaker creates one automatically\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model is training, we can go to the \"SageMaker Experiments and trial\" tab and double-click on the Trial name. There should now be a `DirectMarketingTrainingXGBoost` *Trial component*, which we can double click to further investigate all experiment artifacts including datasets, algorithms, hyperparameters, and model metrics that are being tracked and recorded. This data allows you to trace the complete lineage of a model which helps with model governance, auditing, and compliance verifications.\n",
    "\n",
    "SageMaker Experiments also automatically tracks Amazon SageMaker Autopilot jobs as experiments with their underlying training jobs tracked as trials. You can check how to use SageMaker Autopilot in the [related notebook](sagemaker_autopilot_direct_marketing.ipynb) .\n",
    "\n",
    "Once the training job is completed, you can check the output metrics of the models directly in the SageMaker Experiments interface:\n",
    "\n",
    "![sm-experiment-one-training-metrics.png](./images/sm-experiment-one-training-metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2: attach jobs to an existing Experiment/Trial\n",
    "\n",
    "In the next lab, we’re launching an Hyper-Parameter Optimization job. An HPO job parallelizes launch of multiple training jobs and looks for the best set of hyperparameters given a specific range. At the moment of writing this notebook, SageMaker Tuning jobs do not support the experiment_config parameter, therefore do not log automatically new trial components to SageMaker Experiments. We can perform this operation manually instead. \n",
    "\n",
    "Let’s start by creating the HPO job:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Setup the hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0, 1),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2),\n",
    "    'max_depth': IntegerParameter(1, 10),\n",
    "    'num_round': IntegerParameter(100, 1000)\n",
    "}\n",
    "# Define the target metric and the objective type (max/min)\n",
    "objective_metric_name = 'validation:auc'\n",
    "objective_type='Maximize'\n",
    "# Define the HyperparameterTuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator = xgb,\n",
    "    objective_metric_name = objective_metric_name,\n",
    "    hyperparameter_ranges = hyperparameter_ranges,\n",
    "    objective_type = objective_type,\n",
    "    max_jobs=12,\n",
    "    max_parallel_jobs=4,\n",
    "    early_stopping_type='Auto'\n",
    ")\n",
    "# Launch the HPO job! \n",
    "tuner.fit(\n",
    "    inputs = data_inputs,\n",
    "    job_name = 'DirectMarketing-HPO-XGBoost'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the completion of the HPO job, we can search for the Trial Components that contain the `job_name` we've given to the HPO job previously, and add them to our existing trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "\n",
    "# Create a new Trial just for HPO XGBoost\n",
    "trial_name = f\"{tuner.latest_tuning_job.job_name}-{int(time.time())}\"\n",
    "trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=example_experiment.experiment_name\n",
    ")\n",
    "\n",
    "# Define the search expression: we're looking for\n",
    "# trial components that contain 'DirectMarketing-HPO-XGBoost'\n",
    "search_expression = SearchExpression(\n",
    "    filters=[\n",
    "        Filter('TrialComponentName', Operator.CONTAINS, tuner.latest_tuning_job.job_name),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Let's look for them and add them to our previously created trial\n",
    "trial_component_search_results = TrialComponent.search(search_expression=search_expression)\n",
    "for tc in trial_component_search_results:\n",
    "    trial.add_trial_component(tc.trial_component_name)\n",
    "    time.sleep(0.5)  # sleep to avoid throttling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the results that we've obtained. We can either do this via the SageMaker Experiments and Trials tab, or we can create our own dataframe and plot it according to our preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(sess, sm), \n",
    "    experiment_name=example_experiment.experiment_name,\n",
    "    search_expression={\"Filters\":search_expression.filters},\n",
    "    metric_names=['ObjectiveMetric']\n",
    ")\n",
    "df = trial_component_analytics.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='num_round', y='ObjectiveMetric - Last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 3: Manually logging of metrics and parameters\n",
    "\n",
    "SageMaker Experiments is not just automatic tracking on inputs, parameters, metrics and outputs. It is also possible to use a `smexperiments.tracker.Tracker` object before calling the `fit()` operation to manually record experiment information to a SageMaker trial component:\n",
    "\n",
    "- record one or more parameters with `log_parameter` or `log_parameters`\n",
    "- record one or more input artifacts for this trial component with `log_input` or `log_inputs`\n",
    "- record one or more output artifacts for this trial component with `log_output` or `log_outputs`\n",
    "\n",
    "If you're using your own custom script / container, you can also use SageMaker Experiments to log metrics and other information from within your training loop. You just need to load a `Tracker` and then call the `log_metric` function. \n",
    "\n",
    "```python\n",
    "[...]\n",
    "from smexperiments.tracker import Tracker\n",
    "with Tracker.load() as tracker:\n",
    "    [...]\n",
    "    for epoch in range(epochs):\n",
    "        # your training logic and calculate accuracy and loss\n",
    "        my_tracker.log_metric(metric_name='accuracy', value=0.9, iteration_number=epoch)\n",
    "        my_tracker.log_metric(metric_name='loss', value=0.03, iteration_number=epoch)\n",
    "```\n",
    "\n",
    "Note that metrics logged with this method will only appear in SageMaker when this method is called from a training job host. Learn more at the [SM Experiments documentation](https://sagemaker-experiments.readthedocs.io/en/latest/tracker.html#smexperiments.tracker.Tracker.log_metric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
