{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8079f1d5-633a-4142-9f30-e35dd7e293bf",
   "metadata": {},
   "source": [
    "# Option 2: Batch Inference Pipeline (SageMaker Pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8698d9-a7ee-4094-8a7d-f68d2f3a4fea",
   "metadata": {},
   "source": [
    "- [Overview](#overview)\n",
    "- [Register model into SageMaker Model Registry](#register-model-into-sagemaker-model-registry)\n",
    "   1. [Upload Model Artifact to S3 Bucket](#upload-model-artifact-to-s3-bucket)\n",
    "   2. [Create Model Group](#create-model-group)\n",
    "   3. [Register Model in Model Registry](#register-model-in-model-registry)\n",
    "   4. [Approve Model in Model Registry](#approve-model-in-model-registry)\n",
    "- [Build the pipeline components](#build-the-pipeline-components)\n",
    "   1. [Import statements and declare parameters and constants](#import-statements-and-declare-parameters-and-constants)\n",
    "   2. [Generate Data for Inferences](#generate-data-for-inferences)\n",
    "   3. [Upload Inferences Data to S3 Bucket](#upload-inferences-data-to-s3-bucket)\n",
    "   4. [Info about the Trained Model (An Approved ModelPackage in SageMaker Model Registry)](#info-about-the-trained-model)\n",
    "   5. [Define create model step](#define-create-model-step)\n",
    "   6. [Define Transform Step to Perform Batch Transformation](#define-transform-step-to-perform-batch-transformation)\n",
    "- [Build and Trigger the pipeline run](#build-and-trigger-the-pipeline-run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a3b9e-7668-48d9-be37-14fba386a3ea",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09cae1-4a09-4169-8a2b-5d668823fcac",
   "metadata": {},
   "source": [
    "The following diagram illustrates the high-level architecture of the ML workflow with the different steps to generate inferences using the trained model artifacts. Here we will use offline inferencing to generate predictions\n",
    "\n",
    "![](images/Batch_Inference_Pipeline.png)\n",
    "\n",
    "Inference Pipeline consists of the following steps:\n",
    "\n",
    "1. Create a model in SageMaker using the latest approved model from SageMaker Model Registry.\n",
    "2. Generate Inferences using the trained model  artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e5e88-932a-4be3-87c5-c271d12c5d5a",
   "metadata": {},
   "source": [
    "## Register model into SageMaker Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e31f86-f906-46ca-b901-4502bd8b4067",
   "metadata": {},
   "source": [
    "If running through this lab independently, go through the optional step of uploading the model artifact [customer-retention-model.tar.gz](https://github.com/aws-samples/amazon-sagemaker-immersion-day/blob/master/model/customer-retention-model.tar.gz) into S3 Bucket, registering the model into SageMaker Model Registry and approving the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72edfa-c990-4028-ad49-55c92ccf0580",
   "metadata": {},
   "source": [
    "### Upload Model Artifact to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef277d34-efe2-47b7-89bc-907a6b21a39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker \n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()  \n",
    "default_bucket = sagemaker_session.default_bucket() \n",
    "s3_client = boto3.resource('s3') \n",
    "s3_client.Bucket(default_bucket).upload_file(\"model/customer-retention-model.tar.gz\",\"churn/model_artifacts/customer-retention-model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324de94-ff99-4ea0-933a-c8b7162c6e02",
   "metadata": {},
   "source": [
    "### Create Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d73a9582-5f5d-4978-9167-04cd58b340af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "model_package_group_name = f\"ChurnModelPackageGroup\"\n",
    "region = sagemaker_session.boto_region_name\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "model_package_group_input_dict = {\n",
    " \"ModelPackageGroupName\" : model_package_group_name,\n",
    "}\n",
    "\n",
    "create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n",
    "print('ModelPackageGroup Arn : {}'.format(create_model_package_group_response['ModelPackageGroupArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47100f-5cf5-40fc-a759-8a318d0e6791",
   "metadata": {},
   "source": [
    "### Register Model in Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19594e56-6bfa-4880-bd95-a1d491636bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve the image uri used to train model\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "# Specify the model source\n",
    "model_url = f\"s3://{default_bucket}/churn/model_artifacts/customer-retention-model.tar.gz\"\n",
    "\n",
    "modelpackage_inference_specification =  {\n",
    "    \"InferenceSpecification\": {\n",
    "      \"Containers\": [\n",
    "         {\n",
    "            \"Image\": image_uri,\n",
    "            \"ModelDataUrl\": model_url\n",
    "         }\n",
    "      ],\n",
    "      \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "      \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "   }\n",
    " }\n",
    "\n",
    "# Alternatively, you can specify the model source like this:\n",
    "# modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]=model_url\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\" : model_package_group_name,\n",
    "    \"ModelPackageDescription\" : \"Model to detect 3 different types of irises (Setosa, Versicolour, and Virginica)\",\n",
    "    \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "\n",
    "create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83079d-a7cc-43b6-9f13-5f0ac00ad70b",
   "metadata": {},
   "source": [
    "### Approve Model in Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951008e-9d23-4f5f-b952-ccfd70ce8442",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model registered within model registry can be checked by going to the home screen and choosing the *Models â†’ Model Registry.*\n",
    "\n",
    "![](images/image1.png)\n",
    "\n",
    "you can click on the Update Status tab and manually approve the model.\n",
    "\n",
    "![](images/image2.png)\n",
    "\n",
    "![](images/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f300dea-8931-489b-8bda-9f354434ceb3",
   "metadata": {},
   "source": [
    "## Build the pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf34a2-6166-47ed-8dcb-8e149864356b",
   "metadata": {},
   "source": [
    "### Step 1: Import statements and declare parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be7536c9-9072-4600-9c69-935e07c447a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import pandas as pd \n",
    "import sagemaker \n",
    "from sagemaker.workflow.pipeline_context import PipelineSession \n",
    "\n",
    "s3_client = boto3.resource('s3') \n",
    "pipeline_name = f\"sagemaker-immersion-inference-pipeline\" \n",
    "sagemaker_session = sagemaker.session.Session() \n",
    "region = sagemaker_session.boto_region_name \n",
    "role = sagemaker.get_execution_role() \n",
    "pipeline_session = PipelineSession() \n",
    "default_bucket = sagemaker_session.default_bucket() \n",
    "model_package_group_name = f\"ChurnModelPackageGroup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42fe2cb-7206-4e6d-8474-f3f3b021ed39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ( \n",
    " ParameterInteger, \n",
    " ParameterString, \n",
    " ParameterFloat) \n",
    "\n",
    "base_job_prefix = \"churn-example\"\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString( name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "transform_instance_type = ParameterString(name=\"TransformInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "transform_instance_count = ParameterInteger(name=\"TransformInstanceCount\", default_value=1)\n",
    "batch_data_path = \"s3://{}/data/batch/batch.csv\".format(default_bucket)\n",
    "model_approval_status = ParameterString( name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11dfcf9-b27c-4dd4-bbcb-a72c1aebacf2",
   "metadata": {},
   "source": [
    "### Step 2: Generate Data for Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31a24b-3197-410b-91a7-3f168df1b010",
   "metadata": {
    "tags": []
   },
   "source": [
    "If doing this lab independently, you need to download and save the [_sample dataset_](https://www.kaggle.com/uttamp/store-data) into the project directly within the SageMaker Studio environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8165a7f1-d78e-49f9-9276-44b74e1422e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_batch_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    ## Convert to datetime columns\n",
    "    df[\"firstorder\"]=pd.to_datetime(df[\"firstorder\"],errors='coerce')\n",
    "    df[\"lastorder\"] = pd.to_datetime(df[\"lastorder\"],errors='coerce')\n",
    "    ## Drop Rows with null values\n",
    "    df = df.dropna()\n",
    "    ## Create Column which gives the days between the last order and the first order\n",
    "    df[\"first_last_days_diff\"] = (df['lastorder']-df['firstorder']).dt.days\n",
    "    ## Create Column which gives the days between when the customer record was created and the first order\n",
    "    df['created'] = pd.to_datetime(df['created'])\n",
    "    df['created_first_days_diff']=(df['created']-df['firstorder']).dt.days\n",
    "    ## Drop Columns\n",
    "    df.drop(['custid','created','firstorder','lastorder'],axis=1,inplace=True)\n",
    "    ## Apply one hot encoding on favday and city columns\n",
    "    df = pd.get_dummies(df,prefix=['favday','city'],columns=['favday','city'])\n",
    "    return df\n",
    "    \n",
    "# convert the store_data file into csv format\n",
    "store_data = pd.read_excel(\"storedata_total.xlsx\")\n",
    "store_data.to_csv(\"storedata_total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97cc8374-583e-4518-b073-58b46325cc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess batch data and save into the data folder\n",
    "batch_data = preprocess_batch_data(\"storedata_total.csv\")\n",
    "batch_data.pop(\"retained\")\n",
    "batch_sample = batch_data.sample(frac=0.2)\n",
    "pd.DataFrame(batch_sample).to_csv(\"batch.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dfc3e3-279c-49db-81c0-f5812e84a5c9",
   "metadata": {},
   "source": [
    "### Step 3: Upload Inferences Data to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2215572-0e83-4c35-b4ce-8171a83ae361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.Bucket(default_bucket).upload_file(\"batch.csv\",\"data/batch/batch.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d5996-9e9c-41cd-9706-2efc7b0b19ef",
   "metadata": {},
   "source": [
    "### Step 4:  Info about the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc16c8c6-e677-4dbe-b145-c6df4393a7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\") \n",
    "\n",
    "# get a list of approved model packages from the model package group you specified earlier\n",
    "approved_model_packages = sm_client.list_model_packages(\n",
    "      ModelApprovalStatus='Approved',\n",
    "      ModelPackageGroupName=model_package_group_name,\n",
    "      SortBy='CreationTime',\n",
    "      SortOrder='Descending'\n",
    "  )\n",
    "\n",
    "# find the latest approved model package\n",
    "try:\n",
    "    latest_approved_model_package_arn = approved_model_packages['ModelPackageSummaryList'][0]['ModelPackageArn']\n",
    "except Exception as e:\n",
    "    print(\"Failed to retrieve an approved model package:\", e)\n",
    "    \n",
    "print(latest_approved_model_package_arn) \n",
    "\n",
    " # retrieve required information about the model\n",
    "latest_approved_model_package_descr =  sm_client.describe_model_package(ModelPackageName = latest_approved_model_package_arn)\n",
    "\n",
    "# model artifact uri (tar.gz file)\n",
    "model_artifact_uri = latest_approved_model_package_descr['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "# sagemaker image in ecr\n",
    "image_uri = latest_approved_model_package_descr['InferenceSpecification']['Containers'][0]['Image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe078c-f1b4-4619-aa3f-88806814e227",
   "metadata": {},
   "source": [
    "### Step 5: Define create model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62dbe26b-fdd5-446c-8712-3d05f992a65c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py:261: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "model = Model(\n",
    "image_uri=image_uri,\n",
    "model_data=model_artifact_uri,\n",
    "sagemaker_session=pipeline_session,\n",
    "role=role\n",
    ")\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "name=\"ChurnCreateModel\",\n",
    "step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd0a9-38a7-4fac-b2fc-0493c6508a3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 6: Define Transform Step to Perform Batch Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0980c5f7-c0f7-4c6c-bb7e-2bca34891858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/ChurnTransform\",\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "                                 \n",
    "step_transform = TransformStep(\n",
    "    name=\"ChurnTransform\", \n",
    "    step_args=transformer.transform(\n",
    "                    data=batch_data_path,\n",
    "                    content_type=\"text/csv\"\n",
    "                 )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d93c2d-f4e2-4bd0-af39-0e9a359b095f",
   "metadata": {},
   "source": [
    "## Build and Trigger the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22b01d03-2f8b-42b5-ae28-286040e89dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        transform_instance_type,\n",
    "        transform_instance_count,\n",
    "        batch_data,\n",
    "    ],\n",
    "    steps=[step_create_model,step_transform],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7545be1-c39c-4f8e-bdc5-407da51b6fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "# start Pipeline execution\n",
    "pipeline.start()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
