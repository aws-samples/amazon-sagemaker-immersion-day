{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4024d944-f3fe-4204-9f66-51713557cb41",
   "metadata": {},
   "source": [
    "## Document summarization application with Llama 7B using Amazon SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb731ec6",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16376595",
   "metadata": {},
   "source": [
    "Import the boto3, sagemaker and json modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3945b-3872-4937-8055-0241f0faf6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62921216",
   "metadata": {},
   "source": [
    "Define the sagemaker session and extract the region name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfc105-9468-44ef-ad30-b5e67dcfc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "region_name = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad05346",
   "metadata": {},
   "source": [
    "## 2. Inference with Llama 7B\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d419f2",
   "metadata": {},
   "source": [
    "This function takes a dictionary payload and uses it to invoke the SageMaker runtime client. Then it deserializes the response and prints the input and generated text. The payload includes the prompt as inputs, together with the inference parameters that will be passed to the model. Replace the **endpoint_name** variable with the endpoint name you noted earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453425f3-c7f8-477b-88ce-8008fe57195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "endpoint_name = 'ENDPOINT_NAME'\n",
    "def query_endpoint(payload):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=json.dumps(payload).encode('utf-8'))\n",
    "    model_predictions = json.loads(response['Body'].read())\n",
    "    generated_text = model_predictions[0]['generated_text']\n",
    "    print (\n",
    "        f\"Input Text: {payload['inputs']}{newline}\"\n",
    "        f\"Generated Text: {bold}{generated_text}{unbold}{newline}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a633579f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You can use these parameters with the prompt to tune the output of the model for your use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d82ca2-9518-4faf-a2cf-476b307aa611",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"Peacocktron is obsessed with peacocks, the most glorious bird on the face of this Earth. Peacocktron believes all other birds are irrelevant when compared to the radiant splendor of the peacock. With its iridescent plumage fanning out in a stunning display, the peacock truly stands above the rest. Its regal bearing and dazzling feathers make it the supreme avian specimen. Peacocktron maintains that no other bird can match the peacock's sublime beauty and refuses to waste its time pondering inferior fowl. The peacock reigns supreme in Peacocktron's esteem, for nothing can equal its magnificent and ostentatious elegance..\\nDhiraj: Hello, Peacocktron!\\nPeacocktron:\",\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"return_full_text\": False,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\":10\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485473c3-b42d-4d87-821c-75b1b117f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd3f8f-6cc1-4c7d-a80f-9ebe733c0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"Hello everyone, my name is Dhiraj and  \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d01c5",
   "metadata": {},
   "source": [
    "Now you will use sample research paper to demonstrate summarization. The example text file is concerning automatic text summarization in biomedical literature.Out of the box, the Llama LLM provides support for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43f8b0-5731-48e2-8568-27d87d1a130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"document.txt\") as f:\n",
    "    text_to_summarize = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f7216",
   "metadata": {},
   "source": [
    "## 3. Summarizing with LangChain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f028e",
   "metadata": {},
   "source": [
    "LangChain is an open-source software library that allows developers and data scientists to quickly build, tune, and deploy custom generative applications without managing complex ML interactions, commonly used to abstract many of the common use cases for generative Al language models in just a few lines of code. LangChain's support for AWS services includes support for SageMaker endpoints.\n",
    "LangChain provides an accessible interface to LLMs. Its features include tools for prompt templating and prompt chaining. These chains can be used to summarize text documents that are longer than what the language model supports in a single call. You can use a map-reduce strategy to summarize long documents by breaking it down into manageable chunks, summarizing them, and combining them (and summarized again, if needed).\n",
    "\n",
    "Run next cell to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82ac04-a16e-49d0-bc6a-b2a66fc6553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b46377",
   "metadata": {},
   "source": [
    "Run next cell to import the relevant modules and break down the long document into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df90bd-b6b8-4ba5-b478-552cc9b35805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain import SagemakerEndpoint, PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size = 500,\n",
    "                    chunk_overlap  = 20,\n",
    "                    separators = [\" \"],\n",
    "                    length_function = len\n",
    "                )\n",
    "input_documents = text_splitter.create_documents([text_to_summarize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9246d",
   "metadata": {},
   "source": [
    "To make LangChain work effectively with Llama, you need to define the default content handler classes for valid input and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6619e03-ada8-4c03-9bcc-fbaff6af9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentHandlerTextSummarization(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> json:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        generated_text = response_json[0]['generated_text']\n",
    "        return generated_text.split(\"summary:\")[-1]\n",
    "    \n",
    "content_handler = ContentHandlerTextSummarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471efa6-6bda-44c8-b1ea-bbeac97f273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = \"\"\"Write a concise summary of this text in a few complete sentences:\n",
    "\n",
    "{text}\n",
    "\n",
    "Concise summary:\"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate(\n",
    "                        template=map_prompt, \n",
    "                        input_variables=[\"text\"]\n",
    "                      )\n",
    "\n",
    "\n",
    "combine_prompt = \"\"\"Combine all these following summaries and generate a final summary of them in a few complete sentences:\n",
    "\n",
    "{text}\n",
    "\n",
    "Final summary:\"\"\"\n",
    "\n",
    "combine_prompt_template = PromptTemplate(\n",
    "                            template=combine_prompt, \n",
    "                            input_variables=[\"text\"]\n",
    "                          )      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810e361",
   "metadata": {},
   "source": [
    "LangChain supports LLMs hosted on SageMaker inference endpoints, so instead of using the AWS Python SDK, you can initialize the connection through LangChain for greater accessibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02f914-1866-4564-9ab5-489a87a6becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model = SagemakerEndpoint(\n",
    "                    endpoint_name = endpoint_name,\n",
    "                    region_name= region_name,\n",
    "                    model_kwargs= {},\n",
    "                    content_handler=content_handler\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386dda65",
   "metadata": {},
   "source": [
    "Finally, you can load in a summarization chain and run a summary on the input documents using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d670e-878f-4c7c-b2e1-948ad6ccf7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(llm=summary_model,\n",
    "                                     chain_type=\"map_reduce\", \n",
    "                                     map_prompt=map_prompt_template,\n",
    "                                     combine_prompt=combine_prompt_template,\n",
    "                                     verbose=True\n",
    "                                    ) \n",
    "summary = summary_chain({\"input_documents\": input_documents, 'token_max': 700}, return_only_outputs=True)\n",
    "print(summary[\"output_text\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1e801",
   "metadata": {},
   "source": [
    "Because the verbose parameter is set to True, you'll see all of the intermediate outputs of the map-reduce approach. This is useful for following the sequence of events to arrive at a final summary. With this map-reduce approach, you can effectively summarize documents much longer than is normally allowed by the model's maximum input token limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e912c",
   "metadata": {},
   "source": [
    "## 4. Clean up\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('runtime.sagameker')\n",
    "client.delete_endpoint(EndpointName = endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
